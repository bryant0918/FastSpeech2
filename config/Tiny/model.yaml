transformer:
  encoder_layer: 4
  encoder_head: 2
  encoder_hidden: 256
  decoder_layer: 6
  decoder_head: 2
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.2
  decoder_dropout: 0.2

variance_predictor:
  filter_size: 256
  kernel_size: 3
  dropout: 0.5

variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  n_bins: 256

prosody_extractor:
  dim_in: 2        # Second channel is language embedding
  dim_out: 256
  hidden_dim: 8
  prosody_reg_penalty: 0.001  # Can't be too large, otherwise embedding norms too small and get floating point errors

prosody_predictor:
  sd_dim_in: 512
  si_dim_in: 256
  dim_out: 256
  n_components: 8
  conv_hidden_dim: 8
  gru_hidden_dim: 512
  bigru_hidden_dim: 32

npc:
  in_channels: 256
  hidden_channels: 256
  num_embeddings: 256
  embedding_dim: 256
  input_size: 256
  hidden_size: 512
  n_blocks: 4
  dropout: 0.1
  residual: True
  kernel_size: 15 
  mask_size: 5
  vq: 
    codebook_size: [64,64,64,64]    # Codebook size of each group in VQ-layer
    code_dim: [128,128,128,128] # Dim of each group summing up to hidden_size
    gumbel_temperature: 1.0 
  batch_norm: True
  activate: 'relu'
  disable_cross_layer: False
  dim_bottleneck: null

multi_speaker: False

max_seq_len: 1000

vocoder:
  model: "HiFi-GAN" # support 'HiFi-GAN', 'MelGAN', 'BigVGAN'
  speaker: "universal" # support  'LJSpeech', 'universal'

synthesizer:
  model: "EmotivBeta" # support 'FastSpeech2Pros', 'EmotivBeta' "

transcriber:  # Whisper
  model: "base"  # support 'tiny', 'base', 'small', 'medium', 'large'

sentence_embedder:  # Bert
  model: 'bert-base-uncased'
